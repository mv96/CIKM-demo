@inproceedings{temperature_effect,
  author    = {Matthew Renze},
  editor    = {Yaser Al{-}Onaizan and
               Mohit Bansal and
               Yun{-}Nung Chen},
  title     = {The Effect of Sampling Temperature on Problem Solving in Large Language
               Models},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2024, Miami, Florida, USA, November 12-16, 2024},
  pages     = {7346--7356},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  url       = {https://aclanthology.org/2024.findings-emnlp.432},
  timestamp = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/Renze24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{survey_hallucination,
  author     = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  title      = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  year       = {2025},
  issue_date = {March 2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {43},
  number     = {2},
  issn       = {1046-8188},
  url        = {https://doi.org/10.1145/3703155},
  doi        = {10.1145/3703155},
  abstract   = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
  journal    = {ACM Trans. Inf. Syst.},
  month      = jan,
  articleno  = {42},
  numpages   = {55},
  keywords   = {Large Language Models, Hallucination, Factuality, Faithfulness}
}

@article{hallucination_improvements,
  author     = {Ziwei Xu and
                Sanjay Jain and
                Mohan S. Kankanhalli},
  title      = {Hallucination is Inevitable: An Innate Limitation of Large Language
                Models},
  journal    = {CoRR},
  volume     = {abs/2401.11817},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2401.11817},
  doi        = {10.48550/ARXIV.2401.11817},
  eprinttype = {arXiv},
  eprint     = {2401.11817},
  timestamp  = {Sun, 06 Oct 2024 21:24:38 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2401-11817.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{calibrated_hallucinate,
  author    = {Adam Tauman Kalai and
               Santosh S. Vempala},
  editor    = {Bojan Mohar and
               Igor Shinkar and
               Ryan O'Donnell},
  title     = {Calibrated Language Models Must Hallucinate},
  booktitle = {Proceedings of the 56th Annual {ACM} Symposium on Theory of Computing,
               {STOC} 2024, Vancouver, BC, Canada, June 24-28, 2024},
  pages     = {160--171},
  publisher = {{ACM}},
  year      = {2024},
  url       = {https://doi.org/10.1145/3618260.3649777},
  doi       = {10.1145/3618260.3649777},
  timestamp = {Sun, 19 Jan 2025 13:28:32 +0100},
  biburl    = {https://dblp.org/rec/conf/stoc/KalaiV24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cycles_thought,
  author     = {Evan Becker and
                Stefano Soatto},
  title      = {Cycles of Thought: Measuring {LLM} Confidence through Stable Explanations},
  journal    = {CoRR},
  volume     = {abs/2406.03441},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2406.03441},
  doi        = {10.48550/ARXIV.2406.03441},
  eprinttype = {arXiv},
  eprint     = {2406.03441},
  timestamp  = {Fri, 05 Jul 2024 16:54:14 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2406-03441.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{hallucinations_drug,
  author     = {Shuzhou Yuan and
                Michael F{\"{a}}rber},
  title      = {Hallucinations Can Improve Large Language Models in Drug Discovery},
  journal    = {CoRR},
  volume     = {abs/2501.13824},
  year       = {2025},
  url        = {https://doi.org/10.48550/arXiv.2501.13824},
  doi        = {10.48550/ARXIV.2501.13824},
  eprinttype = {arXiv},
  eprint     = {2501.13824},
  timestamp  = {Thu, 06 Mar 2025 17:19:47 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2501-13824.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{detecting_hallucinations,
  author    = {Sebastian Farquhar and
               Jannik Kossen and
               Lorenz Kuhn and
               Yarin Gal},
  title     = {Detecting hallucinations in large language models using semantic entropy},
  journal   = {Nat.},
  volume    = {630},
  number    = {8017},
  pages     = {625--630},
  year      = {2024},
  url       = {https://doi.org/10.1038/s41586-024-07421-0},
  doi       = {10.1038/S41586-024-07421-0},
  timestamp = {Mon, 09 Dec 2024 22:46:53 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/FarquharKKG24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{defan_dataset,
  author     = {A B. M. Ashikur Rahman and
                Saeed Anwar and
                Muhammad Usman and
                Ajmal Mian},
  title      = {DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation},
  journal    = {CoRR},
  volume     = {abs/2406.09155},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2406.09155},
  doi        = {10.48550/ARXIV.2406.09155},
  eprinttype = {arXiv},
  eprint     = {2406.09155},
  timestamp  = {Tue, 09 Jul 2024 17:23:21 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2406-09155.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ignorance_vs_error,
  author     = {Adi Simhi and
                Jonathan Herzig and
                Idan Szpektor and
                Yonatan Belinkov},
  title      = {Distinguishing Ignorance from Error in {LLM} Hallucinations},
  journal    = {CoRR},
  volume     = {abs/2410.22071},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2410.22071},
  doi        = {10.48550/ARXIV.2410.22071},
  eprinttype = {arXiv},
  eprint     = {2410.22071},
  timestamp  = {Fri, 29 Nov 2024 11:24:24 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2410-22071.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{halueval,
  author    = {Junyi Li and
               Xiaoxue Cheng and
               Xin Zhao and
               Jian{-}Yun Nie and
               Ji{-}Rong Wen},
  editor    = {Houda Bouamor and
               Juan Pino and
               Kalika Bali},
  title     = {HaluEval: {A} Large-Scale Hallucination Evaluation Benchmark for Large
               Language Models},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages     = {6449--6464},
  publisher = {Association for Computational Linguistics},
  year      = {2023},
  url       = {https://doi.org/10.18653/v1/2023.emnlp-main.397},
  doi       = {10.18653/V1/2023.EMNLP-MAIN.397},
  timestamp = {Fri, 12 Apr 2024 13:11:50 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/LiCZNW23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{log_probs_reliability,
  title     = {Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models},
  author    = {Kauf, Carina  and
               Chersoni, Emmanuele  and
               Lenci, Alessandro  and
               Fedorenko, Evelina  and
               Ivanova, Anna A},
  editor    = {Belinkov, Yonatan  and
               Kim, Najoung  and
               Jumelet, Jaap  and
               Mohebbi, Hosein  and
               Mueller, Aaron  and
               Chen, Hanjie},
  booktitle = {Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, US},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.blackboxnlp-1.18/},
  doi       = {10.18653/v1/2024.blackboxnlp-1.18},
  pages     = {263--277},
  abstract  = {Semantic plausibility (e.g. knowing that {\textquotedblleft}the actor won the award{\textquotedblright} is more likely than {\textquotedblleft}the actor won the battle{\textquotedblright}) serves as an effective proxy for general world knowledge. Language models (LMs) capture vast amounts of world knowledge by learning distributional patterns in text, accessible via log probabilities (LogProbs) they assign to plausible vs. implausible outputs. The new generation of instruction-tuned LMs can now also provide explicit estimates of plausibility via prompting. Here, we evaluate the effectiveness of LogProbs and basic prompting to measure semantic plausibility, both in single-sentence minimal pairs (Experiment 1) and short context-dependent scenarios (Experiment 2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a more reliable measure of semantic plausibility than direct zero-shot prompting, which yields inconsistent and often poor results; (ii) instruction-tuning generally does not alter the sensitivity of LogProbs to semantic plausibility (although sometimes decreases it); (iii) across models, context mostly modulates LogProbs in expected ways, as measured by three novel metrics of context-sensitive plausibility and their match to explicit human plausibility judgments. We conclude that, even in the era of prompt-based evaluations, LogProbs constitute a useful metric of semantic plausibility, both in base and instruction-tuned LMs.}
}

@inproceedings{confidence_under_hood,
  author    = {Abhishek Kumar and
               Robert Morabito and
               Sanzhar Umbet and
               Jad Kabbara and
               Ali Emami},
  editor    = {Lun{-}Wei Ku and
               Andre Martins and
               Vivek Srikumar},
  title     = {Confidence Under the Hood: An Investigation into the Confidence-Probability
               Alignment in Large Language Models},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
               August 11-16, 2024},
  pages     = {315--334},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  url       = {https://doi.org/10.18653/v1/2024.acl-long.20},
  doi       = {10.18653/V1/2024.ACL-LONG.20},
  timestamp = {Sun, 19 Jan 2025 13:20:29 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/KumarMUKE24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rouge_k,
  author    = {Sotaro Takeshita and
               Simone Paolo Ponzetto and
               Kai Eckert},
  editor    = {Danushka Bollegala and
               Vered Shwartz},
  title     = {{ROUGE-K:} Do Your Summaries Have Keywords?},
  booktitle = {Proceedings of the 13th Joint Conference on Lexical and Computational
               Semantics, *SEM 2024, Mexico City, Mexico, June 20-21, 2024},
  pages     = {69--79},
  publisher = {Association for Computational Linguistics},
  year      = {2024},
  url       = {https://doi.org/10.18653/v1/2024.starsem-1.6},
  doi       = {10.18653/V1/2024.STARSEM-1.6},
  timestamp = {Mon, 03 Mar 2025 21:22:51 +0100},
  biburl    = {https://dblp.org/rec/conf/starsem/TakeshitaP024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bertscore,
  author    = {Tianyi Zhang and
               Varsha Kishore and
               Felix Wu and
               Kilian Q. Weinberger and
               Yoav Artzi},
  title     = {BERTScore: Evaluating Text Generation with {BERT}},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SkeHuCVFDr},
  timestamp = {Wed, 03 Jun 2020 10:08:32 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{strength_in_numbers,
  title     = {Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement},
  author    = {Portillo Wightman, Gwenyth  and
               Delucia, Alexandra  and
               Dredze, Mark},
  editor    = {Ovalle, Anaelia  and
               Chang, Kai-Wei  and
               Mehrabi, Ninareh  and
               Pruksachatkun, Yada  and
               Galystan, Aram  and
               Dhamala, Jwala  and
               Verma, Apurv  and
               Cao, Trista  and
               Kumar, Anoop  and
               Gupta, Rahul},
  booktitle = {Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.trustnlp-1.28/},
  doi       = {10.18653/v1/2023.trustnlp-1.28},
  pages     = {326--362},
  abstract  = {Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.}
}

@article{lin2004rouge,
  title   = {ROUGE: A package for automatic evaluation of summaries},
  author  = {Lin, Chin-Yew},
  journal = {Text summarization branches out},
  pages   = {74--81},
  year    = {2004}
} 